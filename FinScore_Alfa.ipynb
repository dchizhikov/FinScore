{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Модули"
      ],
      "metadata": {
        "id": "wdV697HpqWyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC6EuEbcqRM6"
      },
      "outputs": [],
      "source": [
        "#import catboost as cb\n",
        "import datetime\n",
        "import gc\n",
        "import lightgbm as lgb\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import seaborn as sns #Graph library: matplot в фоновом режиме\n",
        "import sys\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "from imblearn.pipeline import make_pipeline as imb_make_pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.ensemble import BalancedBaggingClassifier#, EasyEnsemble\n",
        "\n",
        "from joblib import dump, load\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer #Imputer,\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "Imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import (roc_auc_score, confusion_matrix, accuracy_score, roc_curve,\n",
        "                             precision_recall_curve, f1_score, auc)\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "metadata": {
        "id": "RepNNk62qsOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "IjWQJObnrzmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Установки"
      ],
      "metadata": {
        "id": "EVQpJCpXr5Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_TARGET_PATH = \"/content/train_target.csv\"\n",
        "TRAIN_DATA_PATH = \"/content/train_data/\"\n",
        "TEST_DATA_PATH = \"/content/test_data/\"\n",
        "\n",
        "TRAIN_FEATURES_PATH = \"/content/train_features_gb/\"\n",
        "TEST_FEATURES_PATH = \"/content/test_features_gb/\""
      ],
      "metadata": {
        "id": "9f0Xaafzr9Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! rm -r $TRAIN_FEATURES_PATH\n",
        "! mkdir $TRAIN_FEATURES_PATH\n",
        "#! rm -r $TEST_FEATURES_PATH\n",
        "! mkdir $TEST_FEATURES_PATH\n",
        "\n",
        "! mkdir $TRAIN_DATA_PATH\n",
        "! mkdir $TEST_DATA_PATH"
      ],
      "metadata": {
        "id": "DGACJDpKsBMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "id": "kHnnbpNnu5_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "from utils import read_parquet_dataset as read_parquet_dataset_from_local\n",
        "dir(utils)"
      ],
      "metadata": {
        "id": "mcnso-oRvH9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Загрузка"
      ],
      "metadata": {
        "id": "EcFBFv7jsITr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_target = pd.read_csv(\"train_target.csv\")\n",
        "df_parquet = pd.DataFrame()\n",
        "for i in range(5):\n",
        "  df_row = pd.read_parquet(f'{TRAIN_DATA_PATH}train_data_{i}.pq')\n",
        "  df_parquet = pd.concat([df_parquet, df_row], ignore_index=True)\n",
        "  del(df_row)\n",
        "\n",
        "df_parquet = pd.read_parquet('/content/train_data/train_data_0.pq')\n",
        "#df_parquet\n",
        "'''\n",
        "print(df_parquet.nunique())\n",
        "df_parquet=df_parquet#.head(300000)\n",
        "df_parquet#.head(10)\n",
        "'''"
      ],
      "metadata": {
        "id": "ymjS3u5BsMs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target = pd.read_csv(TRAIN_TARGET_PATH)"
      ],
      "metadata": {
        "id": "8xKktRoXvTm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all=df_train_target.merge(df_parquet)"
      ],
      "metadata": {
        "id": "7t1XrRrHuD2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Проверки"
      ],
      "metadata": {
        "id": "PyxTHh1LvqZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.info()\n",
        "df_all.isna().sum().sum()\n",
        "print(df_all.size, df_all.shape)\n",
        "\n",
        "del(df_train_target)\n",
        "del(df_parquet)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "CXqRhs_nwOAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg = df_all[df_all[\"flag\"] == 1].shape[0] #Дефолт\n",
        "pos = df_all[df_all[\"flag\"] == 0].shape[0]\n",
        "print(pos)\n",
        "print(neg)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "index = [0,1]\n",
        "values = [pos,neg]\n",
        "plt.bar(index,values)\n",
        "\n",
        "plt.xticks((0, 1), [\"Ок\", \"Дефолт\"])\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Число заемщиков\")"
      ],
      "metadata": {
        "id": "wLmoVLEzvtyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anext = df_all.loc[:, df_all.columns.isin([\"flag\", 'enc_loans_credit_status'])]#.values\n",
        "print(anext)\n",
        "#anext.plot(x=\"flag\", y=[\"enc_loans_credit_status\"])\n",
        "#plt.plot(anext[\"flag\"], anext[\"enc_loans_credit_status\"])\n",
        "plt.plot(anext[\"enc_loans_credit_status\"], anext[\"flag\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ww79jHw_v0Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_except = [\"id\"] #\"rn\"\n",
        "df_all = df_all.loc[:, ~df_all.columns.isin(col_except)]\n",
        "df_all"
      ],
      "metadata": {
        "id": "jTINmzCcwBw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_importances = [\"flag\", 'pre_loans_total_overdue', 'pre_loans_max_overdue_sum',\n",
        "                   'pre_loans_credit_cost_rate', 'pre_loans_credit_limit',\n",
        "                   'pre_loans_next_pay_summ', 'pre_loans_outstanding',\n",
        "                   'pre_loans90', 'pre_loans6090', 'pre_loans3060', 'pre_loans5',\n",
        "                   'pre_loans530', 'pre_util', 'pre_over2limit', 'pre_maxover2limit',\n",
        "                   'enc_loans_account_holder_type', 'enc_loans_credit_status', 'enc_loans_account_cur',\n",
        "                   'enc_loans_credit_type']\n",
        "\n",
        "df_in = df_all#.loc[:, df_all.columns.isin(col_importances)]\n",
        "df_in"
      ],
      "metadata": {
        "id": "zwJjmFD6wSze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probe_col = 'enc_loans_credit_status'\n",
        "df=df_in[['flag', probe_col]]\n",
        "\n",
        "df_0 = df[df['flag'] == 0].groupby(probe_col).count()\n",
        "df_0.reset_index(inplace=True)\n",
        "df_0.rename(columns = {'flag': 'flag_0'}, inplace = True)\n",
        "\n",
        "df_1 = df[df['flag'] == 1].groupby(probe_col).count()\n",
        "df_1.reset_index(inplace=True)\n",
        "df_1.rename(columns = {'flag': 'flag_1'}, inplace = True)\n",
        "\n",
        "df_01=df_0.merge(df_1, how='outer').fillna(0)\n",
        "df_01['flag_0']=df_01['flag_0'].astype(int)\n",
        "\n",
        "columns=df_01.columns.values.tolist()\n",
        "df_01[columns].plot(x=probe_col, kind='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Quy846bHwZ9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_except = [\"flag\"]\n",
        "\n",
        "X = df.loc[:, ~df.columns.isin(col_except)].values #df.columns != \"flag\"\n",
        "y = df.loc[:, df.columns.isin(col_except)].values.flatten() #df.columns == \"flag\"\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2, shuffle=True,\n",
        "                                                    random_state=0, stratify=y)\n",
        "print(\"Оригинальные размеры данных:\", X_train.shape, X_test.shape)\n",
        "\n",
        "columns=df.loc[:, ~df.columns.isin(col_except)].columns.values.tolist()\n",
        "len(columns)"
      ],
      "metadata": {
        "id": "mj3jtaMRwoZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices_na = np.max(np.isnan(X_train), axis=1)\n",
        "test_indices_na = np.max(np.isnan(X_test), axis=1)\n",
        "X_train_dropna, y_train_dropna = X_train[~train_indices_na, :][:, :-6], y_train[~train_indices_na]\n",
        "X_test_dropna, y_test_dropna = X_test[~test_indices_na, :][:, :-6], y_test[~test_indices_na]\n",
        "print(\"После выкидывания NA: \", X_train_dropna.shape, X_test_dropna.shape)"
      ],
      "metadata": {
        "id": "jC-fMTPOwpwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def importances_indices(rf_clf, df_in):\n",
        "  importances = rf_clf.feature_importances_\n",
        "  indices = np.argsort(importances)[::-1]\n",
        "  skl_imp = pd.Series(importances[indices],\n",
        "                         df_in.drop(['flag'], axis=1).columns[indices])\n",
        "  fig, ax = plt.subplots(figsize=(16,14))\n",
        "  skl_imp.plot.bar(ax=ax)\n",
        "  ax.set_title(\"Важность признаков\")\n",
        "  ax.set_ylabel('Важность')\n",
        "  fig.tight_layout()\n",
        "  print(df_in.drop(['flag'], axis=1).columns[indices])\n",
        "\n",
        "importances_indices(rf_clf, df_in)"
      ],
      "metadata": {
        "id": "vm2VVqxbwsX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Агрегатор"
      ],
      "metadata": {
        "id": "Grpg94l8ufIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CountAggregator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.encoded_features = None\n",
        "\n",
        "    def __extract_count_aggregations(self, data_frame: pd.DataFrame, mode: str) -> pd.DataFrame:\n",
        "        # one-hot-encoding\n",
        "        feature_columns = list(data_frame.columns.values)\n",
        "        feature_columns.remove(\"id\")\n",
        "        feature_columns.remove(\"rn\")\n",
        "\n",
        "        dummies = pd.get_dummies(data_frame[feature_columns], columns=feature_columns)\n",
        "        dummy_features = dummies.columns.values\n",
        "\n",
        "        ohe_features = pd.concat([data_frame, dummies], axis=1)\n",
        "        ohe_features = ohe_features.drop(columns=feature_columns)\n",
        "\n",
        "        # count aggregation\n",
        "        ohe_features.groupby(\"id\")\n",
        "        features = ohe_features.groupby(\"id\")[dummy_features].sum().reset_index(drop=False)\n",
        "        return features\n",
        "\n",
        "    def __transform_data(self, path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=50,\n",
        "                                     mode: str = \"fit_transform\", save_to_path=None, verbose: bool=False):\n",
        "        assert mode in [\"fit_transform\", \"transform\"], f\"Unrecognized mode: {mode}! Please use one of the following modes: \\\"fit_transform\\\", \\\"transform\\\"\"\n",
        "        preprocessed_frames = []\n",
        "        for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
        "                                       desc=\"Transforming sequential credit data\"):\n",
        "            data_frame = read_parquet_dataset_from_local(path_to_dataset, start_from=step,\n",
        "                                                         num_parts_to_read=num_parts_to_preprocess_at_once,\n",
        "                                                         verbose=verbose)\n",
        "            features = self.__extract_count_aggregations(data_frame, mode=mode)\n",
        "            if save_to_path:\n",
        "                features.to_parquet(os.path.join(save_to_path, f\"processed_chunk_{step}.pq\"))\n",
        "            preprocessed_frames.append(features)\n",
        "\n",
        "        features = pd.concat(preprocessed_frames)\n",
        "        features.fillna(np.uint8(0), inplace=True)\n",
        "        dummy_features = list(features.columns.values)\n",
        "        dummy_features.remove(\"id\")\n",
        "        if mode == \"fit_transform\":\n",
        "            self.encoded_features = dummy_features\n",
        "        else:\n",
        "            assert not self.encoded_features is None, \"Transformer not fitted\"\n",
        "            for col in self.encoded_features:\n",
        "                if not col in dummy_features:\n",
        "                    features[col] = np.uint8(0)\n",
        "        return features[[\"id\"]+self.encoded_features]\n",
        "\n",
        "    def fit_transform(self, path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int = 50,\n",
        "                      save_to_path=None, verbose: bool=False):\n",
        "        return self.__transform_data(path_to_dataset=path_to_dataset,\n",
        "                                     num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
        "                                     num_parts_total=num_parts_total, mode=\"fit_transform\",\n",
        "                                     save_to_path=save_to_path, verbose=verbose)\n",
        "    def transform(self, path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=50,\n",
        "                  save_to_path=None, verbose: bool=False):\n",
        "        return self.__transform_data(path_to_dataset=path_to_dataset,\n",
        "                                     num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
        "                                     num_parts_total=num_parts_total, mode=\"transform\",\n",
        "                                     save_to_path=save_to_path, verbose=verbose)"
      ],
      "metadata": {
        "id": "HjWDZ8R6ui_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#из утилс - там ее больше нет\n",
        "\n",
        "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",
        "                                     num_parts_to_read: int = 1, columns=None, verbose=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads num_parts_to_read parquet partitions and returns the resulting pd.DataFrame\n",
        "    :param path_to_dataset: directory with parquet partitions\n",
        "    :param start_from: partition number to start with\n",
        "    :param num_parts_to_read: amount of partitions to read\n",
        "    :param columns: columns to read and include\n",
        "    :return: pd.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    res = []\n",
        "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
        "                              if filename.startswith('part')])\n",
        "\n",
        "    start_from = max(0, start_from)\n",
        "    if num_parts_to_read < 0:\n",
        "        chunks = dataset_paths[start_from: ]\n",
        "    else:\n",
        "        chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
        "    if verbose:\n",
        "        print('Reading chunks:\\n')\n",
        "        for chunk in chunks:\n",
        "            print(chunk)\n",
        "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
        "        chunk = pd.read_parquet(chunk_path, columns=columns)\n",
        "        for col_name, col_type in [('amnt', 'float32'), ('hour_diff', 'int32')]:\n",
        "            if col_name in chunk.columns:\n",
        "                chunk[col_name] = chunk[col_name].astype(col_type)\n",
        "\n",
        "        res.append(chunk)\n",
        "    return pd.concat(res).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WwTwUYzfuoHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "aggregator = CountAggregator()\n",
        "train_data = aggregator.fit_transform(TRAIN_DATA_PATH, num_parts_to_preprocess_at_once=2,\n",
        "                                      num_parts_total=2,\n",
        "                                      save_to_path=TRAIN_FEATURES_PATH, verbose=True)\n",
        "#num_parts_total=12, num_parts_to_preprocess_at_once=4,"
      ],
      "metadata": {
        "id": "FbLJC7nzvCfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Модели"
      ],
      "metadata": {
        "id": "4CE9Do02qdRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Градиентный бустинг"
      ],
      "metadata": {
        "id": "6rpka_x-vVX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "cv = KFold(n_splits=5, random_state=100, shuffle=True)\n",
        "tree_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"max_depth\": 15, #5, 15\n",
        "    \"reg_lambda\": 1,\n",
        "    \"num_leaves\": 64,\n",
        "    \"n_jobs\": -1,#5\n",
        "    \"n_estimators\": 80#2000\n",
        "}"
      ],
      "metadata": {
        "id": "gUEsctZQsOWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "  train_data = pd.read_parquet(TRAIN_DATA_PATH+f'train_data_{i}.pq')\n",
        "  train_data_target = train_target.merge(train_data, on=\"id\")\n",
        "  del(train_data)\n",
        "  gc.collect()\n",
        "  targets = train_data_target[\"flag\"].values\n",
        "  oof = np.zeros(len(train_data_target))\n",
        "  train_preds = np.zeros(len(train_data_target))\n",
        "  feature_cols = list(train_data_target.columns.values)\n",
        "  feature_cols.remove(\"id\"), feature_cols.remove(\"flag\")\n",
        "\n",
        "  for fold_, (train_idx, val_idx) in enumerate(cv.split(train_data_target, targets), 1):\n",
        "      print(f\"Training with fold {fold_} started\")\n",
        "      lgb_model = lgb.LGBMClassifier(**tree_params)\n",
        "  #    lgb_model = RandomForestClassifier(n_estimators = 10, n_jobs=-1, max_features=10,\n",
        "  #                                class_weight=\"balanced\", criterion=\"gini\", warm_start = True)\n",
        "      train, val = train_data_target.iloc[train_idx], train_data_target.iloc[val_idx]\n",
        "\n",
        "  #    lgb_model.fit(RobustScaler().fit_transform(train[feature_cols], train.flag.values),\n",
        "  #               train.flag.values)\n",
        "      lgb_model.fit(train[feature_cols], train.flag.values,\n",
        "                    eval_set=[(val[feature_cols], val.flag.values)],\n",
        "                    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]) #early_stopping_rounds=50, verbose=50\n",
        "\n",
        "      oof[val_idx] = lgb_model.predict_proba(val[feature_cols])[:, 1]\n",
        "      train_preds[train_idx] += lgb_model.predict_proba(train[feature_cols])[:, 1] / (cv.n_splits-1)\n",
        "      models.append(lgb_model)\n",
        "      print(f\"Training with fold {fold_} completed\")\n",
        "  del(train_data_target)\n",
        "  gc.collect()\n",
        "  print(i, \"-----Train roc-auc: \", roc_auc_score(targets, train_preds))\n",
        "  print(i, \"-----CV roc-auc: \", roc_auc_score(targets, oof))\n",
        "del(train_target)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "RMDiWIHQvhNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet = pd.read_parquet(TEST_DATA_PATH + 'test_data_0.pq')\n",
        "df_row = pd.read_parquet(TEST_DATA_PATH + 'test_data_1.pq')\n",
        "test_data = pd.concat([df_parquet, df_row], ignore_index=True)\n",
        "test_data"
      ],
      "metadata": {
        "id": "YDfv2JOkxmlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(df_parquet)\n",
        "del(df_row)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "whnrHbEaxqXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = np.zeros(len(test_data))\n",
        "\n",
        "for model in tqdm.tqdm_notebook(models):\n",
        "    score += model.predict_proba(test_data[feature_cols])[:, 1] / len(models)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\" : test_data[\"id\"].values,\n",
        "    \"score\": score\n",
        "})\n",
        "submission\n",
        "#submission.to_csv(\"submission_lgb.csv\", index=None)\n",
        "#print(\"Train roc-auc: \", roc_auc_score(targets, train_preds))"
      ],
      "metadata": {
        "id": "96c6JL2OxtvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result=submission.groupby('id').mean()\n",
        "df_result.reset_index(inplace=True)\n",
        "df_result['id'] = df_result['id'].astype(int)\n",
        "df_result.to_csv('df_result.csv', index=False)\n",
        "df_result"
      ],
      "metadata": {
        "id": "AWOucjYHxxLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Случайный лес"
      ],
      "metadata": {
        "id": "zaedYPhavaFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_cycle = df_in.shape[0]#100000\n",
        "K_cycle = df_in.shape[0]//n_cycle\n",
        "K_cycle_0 = 0 #random.randrange(0, K_cycle)\n",
        "K_cycle = 1\n",
        "K_cycle\n",
        "n_cycle"
      ],
      "metadata": {
        "id": "9bMVTqS8vdjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc_RandomForestClassifier(rf_clf, X_train_dropna, y_train_dropna, X_test_dropna, y_test_dropna):\n",
        "  #RandomForestClassifier\n",
        "  probas = rf_clf.fit(X_train_dropna, y_train_dropna).predict_proba(X_test_dropna)\n",
        "  fpr, tpr, thresholds = roc_curve(y_test_dropna, probas[:, 1])\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % ('RandomForest', roc_auc))\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.0])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.legend(loc=0, fontsize='small')\n",
        "  plt.show()\n",
        "  return roc_auc\n",
        "#roc_auc_RandomForestClassifier(rf_clf, X_train_dropna, y_train_dropna, X_test_dropna, y_test_dropna)"
      ],
      "metadata": {
        "id": "re1zH9Azw9kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rf_clf = load('filename_01.joblib')\n",
        "print(0, datetime.datetime.now())\n",
        "col_except = [\"flag\"]\n",
        "pip_baseline = make_pipeline(RobustScaler(), rf_clf)\n",
        "\n",
        "for i in range(K_cycle_0, K_cycle_0+K_cycle):\n",
        "  rf_clf = RandomForestClassifier(n_estimators = 50, n_jobs=-1, max_features=10,\n",
        "                                class_weight=\"balanced\", criterion=\"gini\")\n",
        "#criterion=\"entropy\"gini , warm_start = True\n",
        "#  if i > K_cycle_0:\n",
        "#    rf_clf = pickle.loads(rf_clf_save)\n",
        "  X = df_in.iloc[i*n_cycle:(i+1)*n_cycle, ~df_in.columns.isin(col_except)].values #df_in.columns != \"flag\"\n",
        "  y = df_in.iloc[i*n_cycle:(i+1)*n_cycle, df_in.columns.isin(col_except)].values.flatten() #df_in.columns == \"flag\"\n",
        "\n",
        "  X_train_dropna, X_test_dropna, y_train_dropna, y_test_dropna = train_test_split(X, y,\n",
        "    test_size=0.3, shuffle=True, random_state=0, stratify=y)\n",
        "\n",
        "  rf_clf.fit(RobustScaler().fit_transform(Imputer.fit_transform(X_train_dropna)), y_train_dropna)\n",
        "#  scores = cross_val_score(pip_baseline, X_train_dropna, y_train_dropna, scoring=\"roc_auc\", n_jobs=-1, cv=5)\n",
        "#  print(i, \"Среднее значение AUC базовой модели {}\".format(scores.mean()))\n",
        "\n",
        "  #rf_clf_save = pickle.dumps(rf_clf)\n",
        "  filename_model = f'filename_{\"0\"+str(i+1) if i<9 else i+1}.joblib'\n",
        "  dump(rf_clf, filename_model)\n",
        "  files.download(filename_model) #('/content/filename.joblib')\n",
        "  roc_auc = roc_auc_RandomForestClassifier(rf_clf, X_train_dropna, y_train_dropna, X_test_dropna, y_test_dropna)\n",
        "  dt_now = datetime.datetime.now()\n",
        "  print(i+1, dt_now, 'roc_auc =', roc_auc)\n"
      ],
      "metadata": {
        "id": "QdFiVg6yw2kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = load('filename_05.joblib')\n",
        "\n",
        "test = pd.read_csv(\"test_target.csv\")\n",
        "#result = pd.DataFrame(test.id)\n",
        "test_0 = test\n",
        "test"
      ],
      "metadata": {
        "id": "hTcSvtrew6PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet = pd.read_parquet('/content/test_data_0.pq')\n",
        "df_row = pd.read_parquet('/content/test_data_1.pq')\n",
        "df = pd.concat([df_parquet, df_row], ignore_index=True)\n",
        "df"
      ],
      "metadata": {
        "id": "pN6IYxV-xMS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(df_parquet)\n",
        "del(df_row)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "GFu2zcSVxUFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_except = [\"id\", \"rn\"]\n",
        "scores_all = []\n",
        "roc_auc_all = []\n",
        "n_cycle = 100000\n",
        "K_cycle = df.shape[0]//n_cycle\n",
        "print(0, datetime.datetime.now())\n",
        "\n",
        "#for i in range(K_cycle):\n",
        "X_result = df.iloc[:, ~df.columns.isin(col_except)].values\n",
        "\n",
        "# Формируем предсказания для тестовых данных\n",
        "#predictions = rf_clf.predict_proba(test)[:, 1]\n",
        "y_result = rf_clf.predict_proba(X_result) #predict_proba - вероятность, predict - прогноз\n",
        "#  scores = cross_val_score(pip_baseline, X_result, y_result, scoring=\"roc_auc\", cv=10)\n",
        "#  roc_auc_all = roc_auc_all.extend(scores)\n",
        "#  scores_all = scores_all.extend(y_result)\n",
        "#  print(i, \"Среднее значение AUC целевой модели {}\".format(scores.mean()))\n",
        "dt_now = datetime.datetime.now()\n",
        "print(dt_now)\n",
        "#print(i+1, dt_now)\n",
        "print(y_result)"
      ],
      "metadata": {
        "id": "SEQE6b4axVCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_all = y_result[:, 1]\n",
        "#?В ексель надо выводить: GINI = 2 ROC AUC — 1\n",
        "gini_all = [2*i-1 for i in scores_all]\n",
        "\n",
        "scores_df = pd.DataFrame(scores_all, columns =['score'])\n",
        "scores_df\n",
        "\n",
        "s=pd.DataFrame(np.column_stack([df['id'].values, scores_all]), columns =['id', 'score'])\n",
        "s"
      ],
      "metadata": {
        "id": "lCLv4LE7yMTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result=s.groupby('id').mean()\n",
        "df_result.reset_index(inplace= True )\n",
        "df_result['id'] = df_result['id'].astype(int)\n",
        "df_result.to_csv('df_result.csv', index=False)\n",
        "df_result"
      ],
      "metadata": {
        "id": "lgQA8R9ryNVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Прочее"
      ],
      "metadata": {
        "id": "RYiDZna6yXvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_rf = BalancedBaggingClassifier(base_estimator=rf_clf,\n",
        "                                         n_estimators=10,\n",
        "                                         random_state=0)\n",
        "'''\n",
        "pip_resampled = make_pipeline(Imputer(strategy=\"mean\"),\n",
        "                              RobustScaler(),\n",
        "                              resampled_rf)\n",
        "scores = cross_val_score(pip_resampled,\n",
        "                         X_train, y_train,\n",
        "                         scoring=\"roc_auc\", cv=10)\n",
        "\n",
        "print(\"AUC модели EasyEnsemble: \", scores.mean())\n",
        "'''"
      ],
      "metadata": {
        "id": "pkrnX8VUyc8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_rf.fit(X_train_dropna, y_train_dropna)\n",
        "print(y_test_dropna[-3], y_test_dropna[-2])\n",
        "print(resampled_rf.predict([X_test_dropna[-3]]), resampled_rf.predict([X_test_dropna[-2]]))\n",
        "\n",
        "print(y_test_dropna[-3], y_test_dropna[-2])\n",
        "print(resampled_rf.predict([X_test_dropna[-3]]), resampled_rf.predict([X_test_dropna[-2]]))"
      ],
      "metadata": {
        "id": "UJww9iC0yhaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = df_parquet.columns.to_series().groupby(df_parquet.dtypes).groups\n",
        "g\n",
        "\n",
        "df_parquet.groupby('enc_loans_account_cur').mean()\n",
        "print(df_parquet.isnull().sum())"
      ],
      "metadata": {
        "id": "IgIq_7Mbzdjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit = df_parquet.head(10000)\n",
        "sns.countplot(df_credit['pre_loans_credit_cost_rate'])"
      ],
      "metadata": {
        "id": "4EmPnmH6zj-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Результаты"
      ],
      "metadata": {
        "id": "k3aMtFb4yNxV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcpUMMUIyZGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}